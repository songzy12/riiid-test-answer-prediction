{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of SAINT model.\n",
    "\n",
    "Reference:\n",
    "\n",
    "1. https://arxiv.org/abs/2002.07033\n",
    "2. https://www.kaggle.com/claverru/demystifying-transformers-let-s-make-it-public/execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to build data frame.\n",
    "\n",
    "dev = True\n",
    "\n",
    "path_questions = \"../input/riiid-test-answer-prediction/questions.csv\"\n",
    "path_train = \"../input/riiid-test-answer-prediction/train.csv\"\n",
    "\n",
    "dtype_questions = {\n",
    "    \"question_id\": \"int32\",\n",
    "    # 'bundle_id': 'int32',\n",
    "    # 'correct_answer': 'int8',\n",
    "    \"part\": \"int8\",\n",
    "    # 'tags': 'object',\n",
    "}\n",
    "dtype_train = {\n",
    "    \"answered_correctly\": \"int8\",\n",
    "    # 'row_id': 'int64',\n",
    "    # 'timestamp': 'int64',\n",
    "    \"user_id\": \"int32\",\n",
    "    \"content_id\": \"int16\",\n",
    "    # 'content_type_id': 'int8',\n",
    "    \"task_container_id\": \"int16\",\n",
    "    # 'user_answer': 'int8',\n",
    "    \"prior_question_elapsed_time\": \"float32\",\n",
    "    # 'prior_question_had_explanation': 'boolean'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to build data frame.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_question_df(path_questions, dtype_questions):\n",
    "    questions = pd.read_csv(\n",
    "        path_questions,\n",
    "        dtype=dtype_questions,\n",
    "        usecols=dtype_questions.keys(),\n",
    "        index_col=\"question_id\",\n",
    "    )\n",
    "    return questions\n",
    "\n",
    "\n",
    "def get_train_df(path_train, dtype_train, dev):\n",
    "    if dev:\n",
    "        df = pd.read_csv(path_train, usecols=dtype_train.keys(),\n",
    "                         dtype=dtype_train, nrows=10 ** 6)\n",
    "        df = df[df.answered_correctly != -1]\n",
    "        df = df.groupby(\"user_id\").head(1500)\n",
    "    else:\n",
    "        df = pd.read_csv(path_train, usecols=dtype_train.keys(),\n",
    "                         dtype=dtype_train)\n",
    "        df = df[df.answered_correctly != -1]\n",
    "        # TODO(songzy): find a better estimation of head number.\n",
    "        df = df.groupby(\"user_id\").head(1500)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_questions(questions):\n",
    "    part_ids = questions.part.max() + 1\n",
    "    return questions, part_ids\n",
    "\n",
    "\n",
    "def transform_df(df, questions):\n",
    "    df[\"prior_question_elapsed_time\"] = (\n",
    "        df[\"prior_question_elapsed_time\"].fillna(0).astype(np.float32) / 300000\n",
    "    )\n",
    "    content_ids = questions.index.max() + 2\n",
    "    df = df.join(questions, on=\"content_id\")\n",
    "    df[\"content_id\"] += 1\n",
    "    df[\"task_container_id\"] += 1\n",
    "    task_container_ids = 10001\n",
    "    return df, content_ids, task_container_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds data frame.\n",
    "\n",
    "questions = get_question_df(path_questions, dtype_questions)\n",
    "df = get_train_df(path_train, dtype_train, dev)\n",
    "\n",
    "questions, part_ids = transform_questions(questions)\n",
    "df, content_ids, task_container_ids = transform_df(df, questions)\n",
    "\n",
    "df = {uid: u.drop(columns=\"user_id\") for uid, u in df.groupby(\"user_id\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def rolling_window(a, w):\n",
    "    # [1,2,3,4] --- w = 2 --[[1,2], [2,3], [3,4]] but 2D to 3D\n",
    "    s0, s1 = a.strides\n",
    "    m, n = a.shape\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        a, shape=(m - w + 1, w, n), strides=(s0, s0, s1)\n",
    "    )\n",
    "\n",
    "\n",
    "def make_time_series(x, windows_size):\n",
    "    x = np.pad(x, [[windows_size - 1, 0], [0, 0]], constant_values=0)\n",
    "    x = rolling_window(x, windows_size)\n",
    "    return x\n",
    "\n",
    "\n",
    "def add_features_to_user(user):\n",
    "    # We add one to the column in order to have zeros as padding values\n",
    "    # Start Of Sentence (SOS) token will be 3.\n",
    "    user[\"answered_correctly\"] = user[\"answered_correctly\"].shift(\n",
    "        fill_value=2) + 1\n",
    "    return user\n",
    "\n",
    "\n",
    "class RiiidSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, users, windows_size, batch_size=256, start=0, end=None):\n",
    "        self.users = users  # {'user_id': user_df, ...}\n",
    "        self.windows_size = windows_size\n",
    "        # to convert indices to our keys\n",
    "        self.mapper = dict(zip(range(len(users)), users.keys()))\n",
    "        # start and end to easy generate training and validation\n",
    "        self.start = start\n",
    "        self.end = end if end else len(users)\n",
    "        # To know where the answered_correctly_column is\n",
    "        self.answered_correctly_index = list(self.user_example().columns).index(\n",
    "            \"answered_correctly\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid = self.mapper[idx + self.start]\n",
    "        user = self.users[uid].copy()\n",
    "        y = user[\"answered_correctly\"].to_numpy().copy()\n",
    "        x = add_features_to_user(user)\n",
    "        return make_time_series(x, self.windows_size), y\n",
    "\n",
    "    def user_example(self):\n",
    "        \"\"\"Just to check what we have till' now.\"\"\"\n",
    "        uid = self.mapper[self.start]\n",
    "        return add_features_to_user(self.users[uid].copy())\n",
    "\n",
    "    # INFERENCE PART\n",
    "    def get_user_for_inference(self, user_row):\n",
    "        \"\"\"Picks a new user row and concats it to previous interactions\n",
    "        if it was already stored.\n",
    "\n",
    "        Maybe the biggest trick in the notebook is here. We reuse the user_id column to\n",
    "        insert the answered_correctly SOS token because we previously placed the column\n",
    "        there on purpose.\n",
    "\n",
    "        After it, we roll that column and then crop it if it was bigger than the window\n",
    "        size, making the SOS token disapear if out of the sequence.\n",
    "\n",
    "        If the sequence if shorter than the window size, then we pad it.\n",
    "        \"\"\"\n",
    "        uid = user_row[self.answered_correctly_index]\n",
    "        user_row[self.answered_correctly_index] = 2  # SOS token\n",
    "        user_row = user_row[np.newaxis, ...]\n",
    "        if uid in self.users:\n",
    "            x = np.concatenate([self.users[uid], user_row])\n",
    "            # same as in training, we need to add one!!!\n",
    "            x[:, self.answered_correctly_index] = (\n",
    "                np.roll(x[:, self.answered_correctly_index], 1) + 1\n",
    "            )\n",
    "        else:\n",
    "            x = user_row\n",
    "\n",
    "        if x.shape[0] < self.windows_size:\n",
    "            return np.pad(x, [[self.windows_size - x.shape[0], 0], [0, 0]])\n",
    "        elif x.shape[0] > self.windows_size:\n",
    "            return x[-self.windows_size:]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def update_user(self, uid, user):\n",
    "        \"\"\"Concat the new user's interactions to the old ones if already stored.\"\"\"\n",
    "        if uid in self.users:\n",
    "            self.users[uid] = np.concatenate([self.users[uid], user])[\n",
    "                -self.windows_size:\n",
    "            ]\n",
    "        else:\n",
    "            self.users[uid] = user\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis], np.arange(\n",
    "            d_model)[np.newaxis, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# NN THINGS\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += mask * -1e9\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential(\n",
    "        [tf.keras.layers.Dense(dff, activation=\"relu\"),\n",
    "         tf.keras.layers.Dense(d_model)]\n",
    "    )\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "def create_padding_mask(seqs):\n",
    "    # We mask only those vectors of the sequence in which we have all zeroes\n",
    "    # (this is more scalable for some situations).\n",
    "    mask = tf.cast(tf.reduce_all(tf.math.equal(seqs, 0), axis=-1), tf.float32)\n",
    "    # (batch_size, 1, 1, seq_len)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def get_series_model(\n",
    "    n_features,\n",
    "    content_ids,\n",
    "    task_container_ids,\n",
    "    part_ids,\n",
    "    windows_size=64,\n",
    "    d_model=24,\n",
    "    num_heads=4,\n",
    "    n_encoder_layers=2,\n",
    "):\n",
    "    # Input\n",
    "    inputs = tf.keras.Input(shape=(windows_size, n_features), name=\"inputs\")\n",
    "    mask = create_padding_mask(inputs)\n",
    "    pos_enc = positional_encoding(windows_size, d_model)\n",
    "\n",
    "    # Divide branches\n",
    "    content_id = inputs[..., 0]\n",
    "    task_container_id = inputs[..., 1]\n",
    "    answered_correctly = inputs[..., 2]\n",
    "    elapsed_time = inputs[..., 3]\n",
    "    part = inputs[..., 4]\n",
    "\n",
    "    # Create embeddings\n",
    "    content_embeddings = tf.keras.layers.Embedding(\n",
    "        content_ids, d_model)(content_id)\n",
    "    task_embeddings = tf.keras.layers.Embedding(task_container_ids, d_model)(\n",
    "        task_container_id\n",
    "    )\n",
    "    answered_correctly_embeddings = tf.keras.layers.Embedding(4, d_model)(\n",
    "        answered_correctly\n",
    "    )\n",
    "    # Continuous! Only a learnable layer for it.\n",
    "    elapsed_time_embeddings = tf.keras.layers.Dense(d_model, use_bias=False)(\n",
    "        elapsed_time\n",
    "    )\n",
    "    part_embeddings = tf.keras.layers.Embedding(part_ids, d_model)(part)\n",
    "\n",
    "    # Add embeddings\n",
    "    x = tf.keras.layers.Add()(\n",
    "        [\n",
    "            pos_enc,\n",
    "            content_embeddings,\n",
    "            task_embeddings,\n",
    "            answered_correctly_embeddings,\n",
    "            elapsed_time_embeddings,\n",
    "            part_embeddings,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(n_encoder_layers):\n",
    "        x = EncoderLayer(\n",
    "            d_model=d_model, num_heads=num_heads, dff=d_model * 4, rate=0.1\n",
    "        )(x, mask=mask)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "    return tf.keras.Model(inputs, output, name=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model input.\n",
    "\n",
    "train_idx = int(len(df) * 0.8)\n",
    "windows_size = 64\n",
    "\n",
    "s_train = RiiidSequence(df, windows_size, start=0, end=train_idx)\n",
    "s_val = RiiidSequence(df, windows_size, start=train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model.\n",
    "\n",
    "n_features = s_train[0][0].shape[-1]\n",
    "\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "n_encoder_layers = 2\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_series_model(\n",
    "    n_features,\n",
    "    content_ids,\n",
    "    task_container_ids,\n",
    "    part_ids,\n",
    "    windows_size=windows_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "        tf.keras.metrics.BinaryAccuracy(name=\"acc\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04 09:58:54.693204\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3058/3059 [============================>.] - ETA: 0s - loss: 0.6480 - AUC: 0.5849 - acc: 0.6424WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3059/3059 [==============================] - ETA: 0s - loss: 0.6480 - AUC: 0.5849 - acc: 0.6424WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3059/3059 [==============================] - 853s 277ms/step - loss: 0.6480 - AUC: 0.5849 - acc: 0.6424 - val_loss: 0.5841 - val_AUC: 0.7201 - val_acc: 0.6943\n"
     ]
    }
   ],
   "source": [
    "# Trains model.\n",
    "\n",
    "if dev:\n",
    "    epochs = 1\n",
    "else:\n",
    "    epochs = 300\n",
    "patience = 2\n",
    "\n",
    "model.fit(\n",
    "    s_train,\n",
    "    validation_data=s_val,\n",
    "    epochs=epochs,\n",
    "    workers=4,\n",
    "    shuffle=True,\n",
    "    use_multiprocessing=True,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(\n",
    "        patience=patience, monitor=\"val_AUC\", mode=\"max\", restore_best_weights=True\n",
    "    ),\n",
    "    verbose=1,\n",
    ")\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04 10:13:08.127142\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del s_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'riiideducation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2b4a2d6023d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mriiideducation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mriiideducation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miter_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'riiideducation'"
     ]
    }
   ],
   "source": [
    "import riiideducation\n",
    "\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(RiiidSequence(df, windows_size).user_example().columns)\n",
    "columns[columns.index('answered_correctly')] = 'user_id'\n",
    "columns = [c for c in columns if c not in questions.columns] + ['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test, sample_prediction in iter_test:\n",
    "    try:\n",
    "        prior_correct = eval(test['prior_group_answers_correct'].iloc[0])\n",
    "        prior_correct = [a for a in prior_correct if a != -1]\n",
    "    except:\n",
    "        prior_correct = []\n",
    "\n",
    "    # Add prior correct to test and update stored users\n",
    "    if prior_correct:\n",
    "        prior_test.insert(s_train.answered_correctly_index,\n",
    "                          'answered_correctly', prior_correct)\n",
    "        for uid, user in prior_test.groupby('user_id'):\n",
    "            s_train.update_user(\n",
    "                uid, user.drop(columns='user_id').to_numpy())\n",
    "\n",
    "    # Filter test\n",
    "    test = test.loc[\n",
    "        test['content_type_id'] == 0,\n",
    "        columns\n",
    "    ]\n",
    "\n",
    "    # Add global features\n",
    "    test, _, _ = transform_df(test, questions)\n",
    "\n",
    "    # Save test for later\n",
    "    prior_test = test.drop(columns='row_id').copy()\n",
    "\n",
    "    # Make x\n",
    "    x = np.apply_along_axis(\n",
    "        s_train.get_user_for_inference,\n",
    "        1,\n",
    "        test.drop(columns='row_id').to_numpy()\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    test['answered_correctly'] = model.predict(x, batch_size=x.shape[0])\n",
    "\n",
    "    env.predict(test[['row_id', 'answered_correctly']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
